# configs/mllm_config.yaml
# 自己修改
# --- Paths ---
paths:
  output_dir: "checkpoints"
  model_save_name: "mllm_flickr8k_base.pth" # 建议为新模型使用新名称
  # 我们将从 captions 文件构建分词器
  captions_corpus_path: "./data/flickr8k/captions.txt"
  tokenizer_save_path: "checkpoints/flickr8k_tokenizer.json"
  best_model_save_path: 'checkpoints/mllm_flickr8k_base.pth' # 建议为新模型使用新名称

# --- Data ---
data:
  dataset_name: "flickr8k" 
  # 包含 'Images' 文件夹和 'captions.txt' 的根目录
  data_root: "./data/flickr8k" 

model:
  vision_encoder:
    vision_dim: 768     
    n_layers: 6         
    n_heads: 8         
    dropout: 0.1
    d_ff: 1536           
    image_size: 448
    patch_size: 16
    in_channels: 3

  language_model:
    n_layers: 6         
    language_dim: 512    
    n_heads: 8          
    d_ff: 1024           
    max_len: 2048     
    dropout: 0.1

  connector:
    # 由于 vision_dim 和 language_dim 现在相同，连接器主要用于特征对齐
    vision_dim: 768      # 匹配 ViT 的新维度
    language_dim: 512    # 匹配 LLM 的新维度
    type: "mlp"          # 'mlp' 提供了比 'linear' 更强的学习能力
    hidden_dim: 768      # MLP 隐藏层维度，与输入/输出维度保持一致

# --- Training Strategy ---
training:
  # 从头开始训练，所以不冻结任何部分。
  freeze_vit: False
  freeze_llm: False

  # --- Training Hyperparameters ---
  device: "cuda"
  epochs: 30
  batch_size: 4          
  learning_rate: 0.0001  
  num_workers: 4
  eval_interval: 1

# --- Inference ---
generation:
  inference_image_path: ""
  prompt: "" 
  max_new_tokens: 50
  temperature: 0.7